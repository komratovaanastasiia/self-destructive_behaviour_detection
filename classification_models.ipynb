{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TF-IDF"
      ],
      "metadata": {
        "id": "skhCBlmzWEOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest and Logistic Regression"
      ],
      "metadata": {
        "id": "kNVBeH5OX3FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import of necessary libraries"
      ],
      "metadata": {
        "id": "hTb0cGCeFUkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "_BKICL4wyv1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from string import punctuation\n",
        "import re\n",
        "\n",
        "\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "tBEQI-eQGX2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import of data"
      ],
      "metadata": {
        "id": "gZFwG-uoHTxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.read_csv('data.csv')"
      ],
      "metadata": {
        "id": "fgiUM9Ahyhav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15c99862"
      },
      "outputs": [],
      "source": [
        "data_new = new_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test"
      ],
      "metadata": {
        "id": "fQReoT9ozMrp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c01c905"
      },
      "outputs": [],
      "source": [
        "y = data_new.x\n",
        "x=data_new.drop('x',axis=1)\n",
        "x_train, x_test, y_train, y_test =train_test_split(x,y,test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprosessing"
      ],
      "metadata": {
        "id": "of2L97gPHaCS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31125343"
      },
      "outputs": [],
      "source": [
        "def prosessing(text):\n",
        "    #remove html\n",
        "    text = re.sub(r'\\<[^>]*\\>', '', text)\n",
        "    #remove URLs\n",
        "    text = re.sub(r\"https?://[^,\\s]+,?\", \"\", text)\n",
        "    #remove number\n",
        "    text = re.sub(r\"\\d\", \"\", text)\n",
        "    text = re.sub('[^а-яА-Я]', ' ', text)\n",
        "    # tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    #punctuation removal\n",
        "    clean_words = [w.strip(punctuation) for w in tokens]\n",
        "    #lowercase\n",
        "    clean_words = [w.lower() for w in clean_words if w != '']\n",
        "    #str\n",
        "    f1 = \"\"\n",
        "    for i in clean_words:\n",
        "      f1 += str(i)+ \" \" \n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "    words = text.split() \n",
        "    res = list()\n",
        "    for word in words:\n",
        "        p = morph.parse(word)[0]\n",
        "        res.append(p.normal_form)\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "Vg7fGc8_zVCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3751646e"
      },
      "outputs": [],
      "source": [
        "x_train['text'] = x_train['text'].apply(lambda x: prosessing(x))\n",
        "x_train['text'] = x_train['text'].apply(lambda x: lemmatize(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "272c9394"
      },
      "outputs": [],
      "source": [
        "x_test['text'] = x_test['text'].apply(lambda x: prosessing(x))\n",
        "x_test['text'] = x_test['text'].apply(lambda x: lemmatize(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[\"text\"] = x_train[\"text\"].apply(lambda x: ' '.join(x))\n",
        "x_test[\"text\"] = x_test[\"text\"].apply(lambda x: ' '.join(x))\n",
        "\n",
        "x_train"
      ],
      "metadata": {
        "id": "w0ayWvuiHgsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf-idf"
      ],
      "metadata": {
        "id": "VTYiYCV-Hjk0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52e851f0"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "\n",
        "X_train = tfidf_vectorizer.fit_transform(x_train[\"text\"])\n",
        "Y_train = y_train\n",
        "X_test = tfidf_vectorizer.transform(x_test[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data balancing"
      ],
      "metadata": {
        "id": "jgO9IyUdHm4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import NearMiss\n",
        "nm = NearMiss()\n",
        "X_train_miss, Y_train_miss = nm.fit_resample(X_train, Y_train)"
      ],
      "metadata": {
        "id": "p_lZVE3l1jqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=Y_train_miss, data=data, palette='hls')\n",
        "plt.show()\n",
        "plt.savefig('count_plot')"
      ],
      "metadata": {
        "id": "rou4Kd15HzhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_miss, Y_test_miss = nm.fit_resample(X_test_padded, y_test)"
      ],
      "metadata": {
        "id": "yPIY_G3Gj1Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "logistic regression"
      ],
      "metadata": {
        "id": "SqPgL5sdQzdN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f9677f2"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(random_state=0, max_iter=150).fit(X_train_miss, Y_train_miss)\n",
        "predictions1 = clf.predict(X_test_miss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(Y_test_miss, predictions1)"
      ],
      "metadata": {
        "id": "OEK6IMOGH4TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check = []\n",
        "for element in Y_test_miss:\n",
        "    check.append(element)"
      ],
      "metadata": {
        "id": "hQbXImTBzr4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['0', '1']\n",
        "print(classification_report(check, predictions1, target_names=target_names))"
      ],
      "metadata": {
        "id": "HF7iQObmH7Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "random forest"
      ],
      "metadata": {
        "id": "e4o4UUFEQ3SE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52f8c591"
      },
      "outputs": [],
      "source": [
        "rn = RandomForestClassifier(max_depth=2, random_state=0).fit(X_train_miss, Y_train_miss)\n",
        "predictions1 = rn.predict(X_test_miss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(Y_test_miss, predictions1)"
      ],
      "metadata": {
        "id": "cOluD390H9IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check = []\n",
        "for element in Y_test_miss:\n",
        "    check.append(element)"
      ],
      "metadata": {
        "id": "3C999H5iSi0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['0', '1']\n",
        "print(classification_report(check, predictions1, target_names=target_names))"
      ],
      "metadata": {
        "id": "ylH675PGIAZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN and RNN"
      ],
      "metadata": {
        "id": "SMX8zYdxWz1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Embedding,GlobalMaxPool1D, Flatten, Dropout, LSTM, Conv1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "G9zXieSVW4Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import of data"
      ],
      "metadata": {
        "id": "3xMp5f8YXHCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.read_csv('data.csv')"
      ],
      "metadata": {
        "id": "rI66WBEvXHC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahGRdig6XHC_"
      },
      "outputs": [],
      "source": [
        "data_new = new_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prosessing(text):\n",
        "    #remove html\n",
        "    text = re.sub(r'\\<[^>]*\\>', '', text)\n",
        "    #remove URLs\n",
        "    text = re.sub(r\"https?://[^,\\s]+,?\", \"\", text)\n",
        "    #remove number\n",
        "    text = re.sub(r\"\\d\", \"\", text)\n",
        "    text = re.sub('[^а-яА-Я]', ' ', text)\n",
        "    # tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    #punctuation removal\n",
        "    clean_words = [w.strip(punctuation) for w in tokens]\n",
        "    #lowercase\n",
        "    clean_words = [w.lower() for w in clean_words if w != '']\n",
        "    #str\n",
        "    f1 = \"\"\n",
        "    for i in clean_words:\n",
        "      f1 += str(i)+ \" \" \n",
        "    return f1"
      ],
      "metadata": {
        "id": "L1eK1llIyqWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "\n",
        "def lemmatize(text):\n",
        "    words = text.split() \n",
        "    res = list()\n",
        "    for word in words:\n",
        "        p = morph.parse(word)[0]\n",
        "        res.append(p.normal_form)\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "x300eY2RXgHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[\"text\"] = new_df[\"text\"].apply(prosessing)"
      ],
      "metadata": {
        "id": "znPrenH2zg9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[\"text\"] = new_df[\"text\"].apply(lemmatize)"
      ],
      "metadata": {
        "id": "W5vz60-hzj33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[\"text\"] = new_df[\"text\"].apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "01942yN52Kl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = np.asarray(new_df['text'])\n",
        "labels = new_df['x'].to_numpy()\n",
        "\n",
        "X_train, X_test , y_train, y_test = train_test_split(docs, labels , test_size = 0.2, stratify=labels, random_state=42)"
      ],
      "metadata": {
        "id": "bTPpFHVy2DyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocabulary size"
      ],
      "metadata": {
        "id": "jk8NmDB6851i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_counts = Counter()\n",
        "for sent in X_train:\n",
        "    token_counts.update(sent.split(' '))   \n",
        "dict_size = len(token_counts.keys())\n",
        "dict_size"
      ],
      "metadata": {
        "id": "wmYv7INIYTIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=dict_size)\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "wL6JwUUj2DlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tokenized = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_tokenized = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "HOxpGswn2mhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_comment_length = 200\n",
        "X_train_padded = pad_sequences(X_train_tokenized, maxlen=max_comment_length)\n",
        "X_test_padded = pad_sequences(X_test_tokenized, maxlen=max_comment_length)"
      ],
      "metadata": {
        "id": "n0aKZRDf2mdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Балансировка классов"
      ],
      "metadata": {
        "id": "EEnGb_OdZNwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import NearMiss\n",
        "nm = NearMiss()\n",
        "X_train_miss, Y_train_miss = nm.fit_resample(X_train_padded, y_train)"
      ],
      "metadata": {
        "id": "JB9M8uvP2mba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import NearMiss\n",
        "nm = NearMiss()\n",
        "X_test_miss, Y_test_miss = nm.fit_resample(X_test_padded, y_test)"
      ],
      "metadata": {
        "id": "CfI-bPtA3Jhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "WPJty4s8ZSpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = dict_size\n",
        "embedding_dim = 64\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=max_comment_length))\n",
        "model.add(Conv1D(filters=embedding_dim*2, kernel_size=5, activation='relu'))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "txu3EDxSYWap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "\n",
        "history = model.fit(X_train_miss, Y_train_miss, epochs=epochs, validation_data=(X_test_miss, Y_test_miss), batch_size=512)"
      ],
      "metadata": {
        "id": "9JLpr4xoYZ8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Swy3xKfCYcu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test_miss,Y_test_miss)\n",
        "print('Testing Accuracy is {} '.format(accuracy*100))"
      ],
      "metadata": {
        "id": "0o3-lV3IYez4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['loss'], label = 'loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy & loss')\n",
        "plt.ylim([0.0, 1])\n",
        "plt.legend(loc='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dhu5SOsQYgyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = model.predict(X_test_miss, verbose=1)\n",
        "prediction = [1 if i >= .5 else 0 for i in pred_y]"
      ],
      "metadata": {
        "id": "1mpM5TfJYieD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import  classification_report\n",
        "print(classification_report(Y_test_miss, prediction))"
      ],
      "metadata": {
        "id": "jHurTH6z5qtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN (LSTM)"
      ],
      "metadata": {
        "id": "843Ga5dqdXtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = dict_size\n",
        "embedding_dim = 64\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(input_dim=max_features, output_dim=embedding_dim)) \n",
        "model2.add(LSTM(32, return_sequences=True))\n",
        "model2.add(GlobalMaxPool1D())\n",
        "model2.add(Dropout(0.3))\n",
        "model2.add(Dense(64, activation='relu'))\n",
        "model2.add(Dropout(0.3))\n",
        "model2.add(Dense(1, activation = 'sigmoid'))\n",
        "model2.compile(loss='BinaryCrossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "f7k5bVHlYob3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "\n",
        "history2 = model2.fit(X_train_miss, Y_train_miss, epochs=epochs, validation_data=(X_test_miss, Y_test_miss), batch_size=512)"
      ],
      "metadata": {
        "id": "noey29PBYrsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history2.history['loss'])\n",
        "plt.plot(history2.history['val_loss'])\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YWO9Y13yYuHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model2.evaluate(X_test_miss,Y_test_miss)\n",
        "print('Testing Accuracy is {} '.format(accuracy*100))"
      ],
      "metadata": {
        "id": "xMT-5OqMYwVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history2.history['accuracy'], label='accuracy')\n",
        "plt.plot(history2.history['loss'], label = 'loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy & loss')\n",
        "plt.ylim([0.0, 1])\n",
        "plt.legend(loc='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vUIXCP8dYyCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = model2.predict(X_test_miss, verbose=1)\n",
        "prediction = [1 if i >= .5 else 0 for i in pred_y]"
      ],
      "metadata": {
        "id": "4qyRr-WrY0Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import  classification_report\n",
        "print(classification_report(Y_test_miss, prediction))"
      ],
      "metadata": {
        "id": "j1iGvP0Fe2bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Features"
      ],
      "metadata": {
        "id": "IDQ9X6GLWdQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest and Logistic Regression"
      ],
      "metadata": {
        "id": "q1qnjSl-Y7OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature extraction"
      ],
      "metadata": {
        "id": "zAP1f6Tw2YN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "CbHPGi7M92xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import of necessary libraries"
      ],
      "metadata": {
        "id": "zptvPL_-92xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from string import punctuation\n",
        "import re\n",
        "\n",
        "\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "CxK34Bza92xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import of data"
      ],
      "metadata": {
        "id": "ia9JZige92xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.read_csv('data.csv')"
      ],
      "metadata": {
        "id": "fnUWrgAs92xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUtIH5e792xW"
      },
      "outputs": [],
      "source": [
        "data_new = new_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprosessing"
      ],
      "metadata": {
        "id": "PN343yG992xW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqhYHWyb92xW"
      },
      "outputs": [],
      "source": [
        "def prosessing(text):\n",
        "    #remove html\n",
        "    text = re.sub(r'\\<[^>]*\\>', '', text)\n",
        "    #remove URLs\n",
        "    text = re.sub(r\"https?://[^,\\s]+,?\", \"\", text)\n",
        "    #remove number\n",
        "    text = re.sub(r\"\\d\", \"\", text)\n",
        "    text = re.sub('[^а-яА-Я]', ' ', text)\n",
        "    # tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    #punctuation removal\n",
        "    clean_words = [w.strip(punctuation) for w in tokens]\n",
        "    #lowercase\n",
        "    clean_words = [w.lower() for w in clean_words if w != '']\n",
        "    #str\n",
        "    f1 = \"\"\n",
        "    for i in clean_words:\n",
        "      f1 += str(i)+ \" \" \n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "    words = text.split() \n",
        "    res = list()\n",
        "    for word in words:\n",
        "        p = morph.parse(word)[0]\n",
        "        res.append(p.normal_form)\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "9nXym2x592xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lexical diversity"
      ],
      "metadata": {
        "id": "K4i7G2Lp4tOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def counter(text):\n",
        "    li = []\n",
        "    for element in text:\n",
        "        if element not in li:\n",
        "            li.append(element)\n",
        "    return len(li)"
      ],
      "metadata": {
        "id": "aLVnMUi32X_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new['lex_div'] = data_new['proses'].apply(lambda x: counter(x))"
      ],
      "metadata": {
        "id": "DXghjsSu2X7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = []\n",
        "for element in data_new['lex_div']:\n",
        "    count.append(element)\n",
        "    \n",
        "l = []\n",
        "for element in data_new['proses']:\n",
        "    l.append(len(element))"
      ],
      "metadata": {
        "id": "0nels3Qu2X5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verbosity_list = []\n",
        "for element in count:\n",
        "    index = count.index(element)\n",
        "    n = l[index]\n",
        "    d = element/n\n",
        "    verbosity_list.append(d)"
      ],
      "metadata": {
        "id": "rCccRO3z2X3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new = data_new.drop(columns='lex_div')"
      ],
      "metadata": {
        "id": "u2WJDADN2X0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new[\"lex_div\"] = verbosity_list\n",
        "data_new"
      ],
      "metadata": {
        "id": "Y-tmZqPG2XyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "average sentence length"
      ],
      "metadata": {
        "id": "RdVLDnfh5ALW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preposessing_mean(text):\n",
        "    #sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    #len\n",
        "    sentence_word_length = [len(sent.split()) for sent in sentences]\n",
        "    #mean len\n",
        "    mean_sentence_len = np.mean(sentence_word_length)\n",
        "    return mean_sentence_len"
      ],
      "metadata": {
        "id": "rAs0VYMD49yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new['word_sent'] = data_new['text'].apply(lambda x: preposessing_mean(x))"
      ],
      "metadata": {
        "id": "7r6ZAojg49vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I - frequency"
      ],
      "metadata": {
        "id": "qrPbTO255qZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = []\n",
        "for element in data_new['proses']:\n",
        "  c =0\n",
        "  for el in element:\n",
        "    if el == 'я':\n",
        "      c+=1\n",
        "  res = c/len(element)\n",
        "  i.append(res)"
      ],
      "metadata": {
        "id": "zesU4Q2q5p5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new[\"i_freq\"] = i\n",
        "data_new"
      ],
      "metadata": {
        "id": "2iYcme3-49sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS frequecy"
      ],
      "metadata": {
        "id": "N1jK0jpo52Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pos(text):\n",
        "    tags = nltk.pos_tag(text)\n",
        "    return tags"
      ],
      "metadata": {
        "id": "qbaSm2Rk56Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_freq_noun(text):\n",
        "    counter = 0\n",
        "    for element in text:\n",
        "        if element[1] == 'NN' or element[1] == 'NNS' or element[1] == 'NNP' or element[1] == 'NNPS':\n",
        "            counter +=1\n",
        "    a = len(text)\n",
        "    res = counter/a\n",
        "    return res\n",
        "\n",
        "def count_freq_adj(text):\n",
        "    counter = 0\n",
        "    for element in text:\n",
        "        if element[1] == 'JJ' or element[1] == 'JJS' or element[1] == 'JJR':\n",
        "            counter +=1\n",
        "    res = counter/len(text)\n",
        "    return res\n",
        "\n",
        "def count_freq_prepos(text):\n",
        "    counter = 0\n",
        "    for element in text:\n",
        "        if element[1] == 'IN':\n",
        "            counter +=1\n",
        "    res = counter/len(text)\n",
        "    return res\n",
        "\n",
        "def count_freq_pronoun(text):\n",
        "    counter = 0\n",
        "    for element in text:\n",
        "        if element[1] == 'PRP' or element[1] == 'PRP$':\n",
        "            counter +=1\n",
        "    res = counter/len(text)\n",
        "    return res\n",
        "\n",
        "def count_freq_verb(text):\n",
        "    counter = 0\n",
        "    for element in text:\n",
        "        if element[1] == 'VB' or element[1] == 'VBG' or element[1] == 'VBD' or element[1] == 'VBN' or element[1] == 'VBP' or element[1] == 'VBZ':\n",
        "            counter +=1\n",
        "    a = len(text)\n",
        "    res = counter/a\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "6D2-oujn56KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new['pos'] = data_new['proses'].apply(lambda x: pos(x))"
      ],
      "metadata": {
        "id": "DNlb1KK_49qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new['noun'] = data_new['pos'].apply(lambda x: count_freq_noun(x))\n",
        "data_new['verb'] = data_new['pos'].apply(lambda x: count_freq_verb(x))\n",
        "data_new['pronoun'] = data_new['pos'].apply(lambda x: count_freq_pronoun(x))\n",
        "data_new['preposition'] = data_new['pos'].apply(lambda x: count_freq_prepos(x))\n",
        "data_new['adjective'] = data_new['pos'].apply(lambda x: count_freq_adj(x))"
      ],
      "metadata": {
        "id": "Emy-999x6GuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "readability"
      ],
      "metadata": {
        "id": "IWxQyj4v6Lox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ruts"
      ],
      "metadata": {
        "id": "STnMlhkZ6GnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ruts import ReadabilityStats\n",
        "rs = ReadabilityStats(text)\n",
        "redabl = []\n",
        "for element in data_new['text']:\n",
        "  rs = ReadabilityStats(element)\n",
        "  r = rs.flesch_reading_easy\n",
        "  redabl.append(r)"
      ],
      "metadata": {
        "id": "GrqtokVE6GdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new[\"redabl\"] = redabl\n",
        "data_new"
      ],
      "metadata": {
        "id": "R5dFZFW_6YTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentiment"
      ],
      "metadata": {
        "id": "I-UZJIED6fZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dostoevsky"
      ],
      "metadata": {
        "id": "3aAu8V866fIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m dostoevsky download fasttext-social-network-model"
      ],
      "metadata": {
        "id": "1Q6l0tXq6fE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dostoevsky.tokenization import RegexTokenizer\n",
        "from dostoevsky.models import FastTextSocialNetworkModel\n",
        "tokenizer = RegexTokenizer()\n",
        "model = FastTextSocialNetworkModel(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "ul7HU7N76fCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = data_new['text']\n",
        "results = model.predict(messages, k=2)\n",
        "data_new['polarity'] = results"
      ],
      "metadata": {
        "id": "Tsnga2LB6vTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "el = 0\n",
        "for element in data_new['text']:\n",
        "  x = len(element.split(' '))\n",
        "  el+=x"
      ],
      "metadata": {
        "id": "xIC7xtKZ6vPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = []\n",
        "for element in data_new['polarity']:\n",
        "  x = max(element, key=element.get)\n",
        "  f.append(x)"
      ],
      "metadata": {
        "id": "btUEjl2L6vNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new['sentiment'] = f\n",
        "data_new"
      ],
      "metadata": {
        "id": "Fa2XIkwG6fAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new['y'] = pd.factorize(data_new['sentiment'])[0]"
      ],
      "metadata": {
        "id": "rBcfVqCX627g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation analysis of the dataset"
      ],
      "metadata": {
        "id": "_Zf2tYZKDSOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = data_new.corr()\n",
        "corr_matrix"
      ],
      "metadata": {
        "id": "itzacbKbDZC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix[\"x\"].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "JuKS_9S7DXMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization"
      ],
      "metadata": {
        "id": "CXimr-0DZ__6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "minmax = MinMaxScaler()"
      ],
      "metadata": {
        "id": "17DJOOOK-LcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "da = data_new.copy()"
      ],
      "metadata": {
        "id": "YrqIk8Rf-Ugx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "da['lex_div'] = minmax.fit_transform(np.array(da['lex_div']).reshape(-1, 1))\n",
        "da['word_sent'] = minmax.fit_transform(np.array(da['word_sent']).reshape(-1, 1))\n",
        "da['i_freq'] = minmax.fit_transform(np.array(da['i_freq']).reshape(-1, 1))\n",
        "da['preposition'] = minmax.fit_transform(np.array(da['preposition']).reshape(-1, 1))\n",
        "da['pronoun'] = minmax.fit_transform(np.array(da['pronoun']).reshape(-1, 1))\n",
        "da['verb'] = minmax.fit_transform(np.array(da['verb']).reshape(-1, 1))\n",
        "da['noun'] = minmax.fit_transform(np.array(da['noun']).reshape(-1, 1))\n",
        "da['y'] = minmax.fit_transform(np.array(da['y']).reshape(-1, 1))\n",
        "da['adjective'] = minmax.fit_transform(np.array(da['adjective']).reshape(-1, 1))\n",
        "da['redabl'] = minmax.fit_transform(np.array(da['redabl']).reshape(-1, 1))"
      ],
      "metadata": {
        "id": "KWY8yVKP-RcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test"
      ],
      "metadata": {
        "id": "OO4_NlLeBwne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg0jslPNA5vF"
      },
      "outputs": [],
      "source": [
        "y = da.x\n",
        "x=da.drop('x',axis=1)\n",
        "x_train, x_test, y_train, y_test =train_test_split(x,y,test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "random forest"
      ],
      "metadata": {
        "id": "_JP4WxZEALM6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trHPd0tP_9co"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(max_depth=2, random_state=0).fit(x_train, y_train)\n",
        "predictions1 = clf.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check = []\n",
        "for element in y_test:\n",
        "    check.append(element)"
      ],
      "metadata": {
        "id": "HqCTCKRPALM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['0', '1']\n",
        "print(classification_report(check, predictions1, target_names=target_names))"
      ],
      "metadata": {
        "id": "Psw3VQl7ALM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "logistic regression"
      ],
      "metadata": {
        "id": "jp6r5eS1ALMq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxiSWboEX6hq"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(random_state=0, max_iter=150).fit(df_train, y_train)\n",
        "predictions1 = clf.predict(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0wNfbgw_9cp"
      },
      "outputs": [],
      "source": [
        "check = []\n",
        "for element in y_test:\n",
        "    check.append(element)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['0', '1']\n",
        "print(classification_report(check, predictions1, target_names=target_names))"
      ],
      "metadata": {
        "id": "jvJcb6jVZGTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN and RNN"
      ],
      "metadata": {
        "id": "nzsOHVd3ZIbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "res = preprocessing.normalize(da, axis=0)"
      ],
      "metadata": {
        "id": "8wljduKuZLtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = res\n",
        "labels = da['x'].to_numpy()\n",
        "\n",
        "X_train, X_test , y_train, y_test = train_test_split(docs, labels , test_size = 0.2, stratify=labels, random_state=42)"
      ],
      "metadata": {
        "id": "YDmCcUnmtWUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import NearMiss\n",
        "nm = NearMiss()\n",
        "X_train_miss, Y_train_miss = nm.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "k0U5cdg2uzqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import NearMiss\n",
        "nm = NearMiss()\n",
        "X_test_miss, Y_test_miss = nm.fit_resample(X_test, y_test)"
      ],
      "metadata": {
        "id": "0QabYN4ovK4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "1nzolXpzZwY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 9\n",
        "embedding_dim = 64\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=max_comment_length))\n",
        "model.add(Conv1D(filters=embedding_dim*2, kernel_size=5, activation='relu'))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "EzxoriN2ZwY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "\n",
        "history = model.fit(X_train_miss, Y_train_miss, epochs=epochs, validation_data=(X_test_miss, Y_test_miss), batch_size=512)"
      ],
      "metadata": {
        "id": "85uODCffZwY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = model.predict(X_test_miss, verbose=1)\n",
        "prediction = [1 if i >= .5 else 0 for i in pred_y]"
      ],
      "metadata": {
        "id": "kHLrlR7RZwZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import  classification_report\n",
        "print(classification_report(Y_test_miss, prediction))"
      ],
      "metadata": {
        "id": "B9sFvnCCZwZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN (LSTM)"
      ],
      "metadata": {
        "id": "emrKn68UZwZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 9\n",
        "embedding_dim = 64\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(input_dim=max_features, output_dim=embedding_dim)) \n",
        "model2.add(LSTM(32, return_sequences=True))\n",
        "model2.add(GlobalMaxPool1D())\n",
        "model2.add(Dropout(0.3))\n",
        "model2.add(Dense(64, activation='relu'))\n",
        "model2.add(Dropout(0.3))\n",
        "model2.add(Dense(1, activation = 'sigmoid'))\n",
        "model2.compile(loss='BinaryCrossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "Mf1yDGKrZg7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "\n",
        "history2 = model2.fit(X_train_miss, Y_train_miss, epochs=epochs, validation_data=(X_test_miss, Y_test_miss), batch_size=512)"
      ],
      "metadata": {
        "id": "A_BIzBPeZwZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = model2.predict(X_test_miss, verbose=1)\n",
        "prediction = [1 if i >= .5 else 0 for i in pred_y]"
      ],
      "metadata": {
        "id": "J7O7Ai7sZwZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import  classification_report\n",
        "print(classification_report(Y_test_miss, prediction))"
      ],
      "metadata": {
        "id": "7EEvkaqKZwZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT"
      ],
      "metadata": {
        "id": "fcHh4N47JREA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "4ecaJ1VfJSIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import load_metric, Dataset\n",
        "from sklearn.metrics import classification_report, f1_score"
      ],
      "metadata": {
        "id": "eBrHKIJ-eiQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "seed_all(42)"
      ],
      "metadata": {
        "id": "i3NqNy1PfCpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "isSv1076f6Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading a pre-trained model"
      ],
      "metadata": {
        "id": "hS7BauifKM8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased-sentence', num_labels=2)\n",
        "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased-sentence')"
      ],
      "metadata": {
        "id": "x0qjMCFbfGd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new = new_df.copy()"
      ],
      "metadata": {
        "id": "sHDhIgcShDwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test\n",
        "y = data_new.x\n",
        "x=data_new.drop('x',axis=1)\n",
        "x_train, x_test, y_train, y_test =train_test_split(x,y,test_size=0.2)"
      ],
      "metadata": {
        "id": "EJcNL65Kg_EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = x_train['text'].astype('str')"
      ],
      "metadata": {
        "id": "HWhDGlnOhYwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = y_train"
      ],
      "metadata": {
        "id": "_WXtHZfQhh6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = x_test['text'].astype('str')"
      ],
      "metadata": {
        "id": "H1lu9uRuhvnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = y_test"
      ],
      "metadata": {
        "id": "ui6LibJChz-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len_train = [len(str(i).split()) for i in x_train['text']]\n",
        "seq_len_test = [len(str(i).split()) for i in x_test['text']]\n",
        "max_seq_len = max(max(seq_len_test), max(seq_len_train))\n",
        "max_seq_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl1d6er6gmcc",
        "outputId": "2dd9a1a5-3ecd-4196-95bc-c595225cc9c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3195967"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 512"
      ],
      "metadata": {
        "id": "1Ft0bcRWj0eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.values,\n",
        "    max_length = max_seq_len,\n",
        "    padding = 'max_length',\n",
        "    truncation = True\n",
        ")\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.values,\n",
        "    max_length = max_seq_len,\n",
        "    padding = 'max_length',\n",
        "    truncation = True\n",
        ")"
      ],
      "metadata": {
        "id": "98X1jOPUiK3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "train_dataset = Data(tokens_train, train_labels)\n",
        "test_dataset = Data(tokens_test, test_labels)"
      ],
      "metadata": {
        "id": "XTI1obM9mlDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = f1_score(labels, preds)\n",
        "    return {'F1': f1}"
      ],
      "metadata": {
        "id": "JpwHTBXymo8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6Pc4tCknGoq",
        "outputId": "e9393898-7bf5-48c2-9a82-6d5909183a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = '/results', #Выходной каталог\n",
        "    num_train_epochs = 3, #Кол-во эпох для обучения\n",
        "    per_device_train_batch_size = 8, #Размер пакета для каждого устройства во время обучения\n",
        "    per_device_eval_batch_size = 8, #Размер пакета для каждого устройства во время валидации\n",
        "    weight_decay =0.01, #Понижение весов\n",
        "    logging_dir = './logs', #Каталог для хранения журналов\n",
        "    load_best_model_at_end = True, #Загружать ли лучшую модель после обучения\n",
        "    learning_rate = 1e-5, #Скорость обучения\n",
        "    evaluation_strategy ='epoch', #Валидация после каждой эпохи (можно сделать после конкретного кол-ва шагов)\n",
        "    logging_strategy = 'epoch', #Логирование после каждой эпохи\n",
        "    save_strategy = 'epoch', #Сохранение после каждой эпохи\n",
        "    save_total_limit = 1,\n",
        "    seed=21)"
      ],
      "metadata": {
        "id": "HjpN6tfmmsfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model,\n",
        "                  tokenizer = tokenizer,\n",
        "                  args = training_args,\n",
        "                  train_dataset = train_dataset,\n",
        "                  eval_dataset = train_dataset,\n",
        "                  compute_metrics = compute_metrics)"
      ],
      "metadata": {
        "id": "gumpEcYdnCNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "AAcosKSkc2-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction():\n",
        "    test_pred = trainer.predict(test_dataset)\n",
        "    labels = np.argmax(test_pred.predictions, axis = -1)\n",
        "    return labels\n",
        "pred = get_prediction()"
      ],
      "metadata": {
        "id": "urS3wasic4Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_labels, pred))\n",
        "print(f1_score(test_labels, pred))"
      ],
      "metadata": {
        "id": "mt94ytetc8sC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}